{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06373fb6",
   "metadata": {},
   "source": [
    "# Speech Analysis in conversation in computer science Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9605ef01",
   "metadata": {},
   "source": [
    "### Initial Plan of Attack :\n",
    "    Step 1 : Audio to Text File Conversion\n",
    "    Step 2 : Cleaning the data\n",
    "    Step 3 : Emotion Detection\n",
    "    Step 4 : Topic Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc62bc1",
   "metadata": {},
   "source": [
    "#### Firstly Lets Just Install the libraries Required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303823c",
   "metadata": {},
   "source": [
    "Text2Emotion is a python package designed to identify emotions in text data. It works by recognizing emotions expressed in words when people are confident in their statements. For example, a dissatisfied customer may say, \"I am very angry by your product services and gonna file a complaint.\" Text2Emotion can extract emotions from text and categorize them as Happy, Angry, Sad, Surprise, or Fear, providing a dictionary output.\n",
    "\n",
    "#### https://pypi.org/project/text2emotion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84aecd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: text2emotion in c:\\users\\mourya kunuku\\appdata\\roaming\\python\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: nltk in c:\\anaconda\\lib\\site-packages (from text2emotion) (3.7)\n",
      "Requirement already satisfied: emoji>=0.6.0 in c:\\users\\mourya kunuku\\appdata\\roaming\\python\\python39\\site-packages (from text2emotion) (1.7.0)\n",
      "Requirement already satisfied: click in c:\\anaconda\\lib\\site-packages (from nltk->text2emotion) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\anaconda\\lib\\site-packages (from nltk->text2emotion) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\anaconda\\lib\\site-packages (from nltk->text2emotion) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from nltk->text2emotion) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mourya kunuku\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk->text2emotion) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install text2emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5489e",
   "metadata": {},
   "source": [
    "NeatText is a simple NLP package for cleaning textual data and text preprocessing. Simplifying Text Cleaning For NLP & ML\n",
    "\n",
    "#### https://pypi.org/project/neattext/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1374617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: neattext in c:\\users\\mourya kunuku\\appdata\\roaming\\python\\python39\\site-packages (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "pip install neattext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0dde6",
   "metadata": {},
   "source": [
    "#### Please find all the TV show conversations links i used as audio files in the project\n",
    "\n",
    "#### https://www.youtube.com/watch?v=mNIXRXikYDc&ab_channel=TheEllenShow\n",
    "#### https://www.youtube.com/watch?v=hVd_rdhKTVk&ab_channel=AppleTV\n",
    "#### https://www.youtube.com/watch?v=skj-ALA1HFE&ab_channel=VideoAdvice\n",
    "#### https://www.youtube.com/watch?v=PNTCM7cbrsc&t=184s&ab_channel=CampusMovieFest\n",
    "#### https://www.youtube.com/watch?v=uQDDGriA1lk&ab_channel=SteveTVShow\n",
    "#### https://www.youtube.com/watch?v=f5NJQiY9AuY&t=12s&ab_channel=TheEllenShow\n",
    "#### https://www.youtube.com/watch?v=sd7dSHU4BKs&t=12s&ab_channel=Simulation\n",
    "#### https://www.youtube.com/watch?v=yRQ5ntxnFaI&t=2s&ab_channel=TheLateShowwithStephenColbert\n",
    "#### https://www.youtube.com/watch?v=pKtNyN53B_s&t=2s&ab_channel=TheKellyClarksonShow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bba08f",
   "metadata": {},
   "source": [
    "####                                                            Step 1 : Audio to Text File Conversion\n",
    "\n",
    "This code is a Python script that uses the AssemblyAI API to upload an audio file, transcribe it, and return the text transcript.\n",
    "\n",
    "The script uses the requests library to make HTTP requests to the API, and defines three functions:\n",
    "\n",
    "    uploadMyFile to upload the audio file to AssemblyAI and return an upload URL.\n",
    "    startTranscription to initiate the transcription process using the upload URL.\n",
    "    getTranscription to check the status of the transcription and retrieve the text transcript once it is completed.\n",
    "    \n",
    "The script requires an authentication key to use the AssemblyAI API, which must be obtained from the AssemblyAI website. The key is stored in the authKey variable.\n",
    "\n",
    "#### https://www.assemblyai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a670b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make requests to the API\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Step 1\n",
    "# Register and get auth key from https://www.assemblyai.com/\n",
    "authKey = 'f9d89818e1214415b6dd6c9642439a6a'\n",
    "\n",
    "# Parameters for HTTP request\n",
    "headers = {\n",
    "    'authorization' : authKey,\n",
    "    'content-type'  : 'application/json'\n",
    "}\n",
    "\n",
    "# Url's for Upload and transcripts assigned by assemblyai\n",
    "uploadUrl      = 'https://api.assemblyai.com/v2/upload'\n",
    "transcriptUrl  = 'https://api.assemblyai.com/v2/transcript'\n",
    "\n",
    "# Step 2 : Upload audio file\n",
    "def uploadMyFile(fileName):\n",
    "\n",
    "    def _readMyFile(fn):\n",
    "        # \n",
    "        chunkSize = 10#5242880\n",
    "        \n",
    "        # Creation of Filestream to Read the file\n",
    "        with open(fn, 'rb') as fileStream:\n",
    "\n",
    "            while True:\n",
    "                data = fileStream.read(chunkSize)\n",
    "                ## Since it is a while we need to end the loop when data is not present\n",
    "                if not data:\n",
    "                    break\n",
    "                ##  we use yeild instead of return for returning the whole file \n",
    "                ##  not just the first chunk of the file becasue we are reading the file in chunks\n",
    "                yield data\n",
    "    ## POST method\n",
    "    response = requests.post(\n",
    "        uploadUrl,\n",
    "        headers= headers,\n",
    "        data= _readMyFile(fileName)\n",
    "    )\n",
    "    \n",
    "    ## every response has a json so we are intializing it to json\n",
    "    json = response.json()\n",
    "\n",
    "    return json['upload_url']\n",
    "# END def uploadMyFile\n",
    "\n",
    "# Step 3 : Start Transcription\n",
    "## we provide the uploaded audio url and expect transcription Id from assemblyai\n",
    "def startTranscription(aurl):\n",
    "    ## POST method\n",
    "    response = requests.post(\n",
    "        transcriptUrl,\n",
    "        headers= headers,\n",
    "        json= { 'audio_url' : aurl }\n",
    "    )\n",
    "    ## every response has a json so we are intializing it to json\n",
    "    json = response.json()\n",
    "\n",
    "    return json['id']\n",
    "# END def startTranscription\n",
    "\n",
    "\n",
    "\n",
    "# Step 4 : Start Transcription\n",
    "## we provide the transcription Id and expect text from assemblyai\n",
    "def getTranscription(tid):\n",
    "\n",
    "    maxAttempts = 1000\n",
    "    timedout    = False\n",
    "\n",
    "    while True:\n",
    "        ## Get method\n",
    "        response = requests.get(\n",
    "            f'{transcriptUrl}/{tid}', #transcriptUrl + '/' + tid,\n",
    "            headers= headers\n",
    "        )\n",
    "        \n",
    "        ## every response has a json so we are intializing it to json\n",
    "        json = response.json()\n",
    "        \n",
    "        ## condition to break out of while loop\n",
    "        if json['status'] == 'completed':\n",
    "            break\n",
    "\n",
    "        maxAttempts -= 1\n",
    "        timedout = maxAttempts <= 0\n",
    "\n",
    "        if timedout:\n",
    "            break\n",
    "\n",
    "        # Wait for 3 seconds before make the next try!\n",
    "        # Why? Because we don't want to set AssemblyAI on Fire!!!\n",
    "        time.sleep(3)\n",
    "\n",
    "    return 'Timeout...' if timedout else json['text']\n",
    "# END def getTranscription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8748f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filenames = []\n",
    "audio_transcript = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311d9548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mourya\n",
      "[nltk_data]     Kunuku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Mourya\n",
      "[nltk_data]     Kunuku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mourya\n",
      "[nltk_data]     Kunuku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import the modules\n",
    "import text2emotion as te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf199642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing : .\\audio\\Bill_Gates_Chats_with_Ellen_for_the_First_Time.mp3\n",
      "processing Completed: .\\audio\\Bill_Gates_Chats_with_Ellen_for_the_First_Time.mp3\n",
      "processing : .\\audio\\Ellen_Taught_This_Fan_How_to_Speak_English.mp3\n",
      "processing Completed: .\\audio\\Ellen_Taught_This_Fan_How_to_Speak_English.mp3\n",
      "processing : .\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Strange'_Intimate_Scenes_In_'Conversations_With_Friends'.mp3\n",
      "processing Completed: .\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Strange'_Intimate_Scenes_In_'Conversations_With_Friends'.mp3\n",
      "processing : .\\audio\\Jordan_Peterson___How_to_Have_Better_Conversations.mp3\n",
      "processing Completed: .\\audio\\Jordan_Peterson___How_to_Have_Better_Conversations.mp3\n",
      "processing : .\\audio\\Penn_Badgley_Can_Go_From_Charming_To_Creepy_Without_Changing_His_Expression.mp3\n",
      "processing Completed: .\\audio\\Penn_Badgley_Can_Go_From_Charming_To_Creepy_Without_Changing_His_Expression.mp3\n",
      "processing : .\\audio\\Small_Talk.mp3\n",
      "processing Completed: .\\audio\\Small_Talk.mp3\n",
      "processing : .\\audio\\The_Oprah_Conversation_—_Will_Smith_On_His_Marriage_to_Jada___Apple_TV+.mp3\n",
      "processing Completed: .\\audio\\The_Oprah_Conversation_—_Will_Smith_On_His_Marriage_to_Jada___Apple_TV+.mp3\n",
      "processing : .\\audio\\The_Viral_Voice_that_Sounds_Like_Siri____STEVE_HARVEY.mp3\n",
      "processing Completed: .\\audio\\The_Viral_Voice_that_Sounds_Like_Siri____STEVE_HARVEY.mp3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "## Function to get all the file names whose extension ends with .mp3\n",
    "def mp3gen():\n",
    "    for root, dirs, files in os.walk('.'):\n",
    "        for filename in files:\n",
    "            if os.path.splitext(filename)[1] == \".mp3\":\n",
    "                yield os.path.join(root, filename)\n",
    "\n",
    "for mp3file in mp3gen():\n",
    "    print(f\"processing : {mp3file}\")\n",
    "    audioUrl = uploadMyFile(mp3file)\n",
    "\n",
    "    # step 2) Start Transcription\n",
    "    transcriptionID = startTranscription(audioUrl)\n",
    "\n",
    "    # step 3) Get Transcription Text\n",
    "    text = getTranscription(transcriptionID)\n",
    "    \n",
    "    ## Adding Filenames and Audio Transcript to lists\n",
    "    audio_filenames.append(mp3file)\n",
    "    audio_transcript.append(text)\n",
    "    \n",
    "    print(f\"processing Completed: {mp3file}\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ae3fd",
   "metadata": {},
   "source": [
    "#### Step 2 : Cleaning the data and Step 3 : Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97c7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neattext.functions as nfx\n",
    "## Emotions list to store all the emotion results from each audio\n",
    "emotions = []\n",
    "##  To store the cleaned text record for each audio\n",
    "cleaned_audiotext = []\n",
    "\n",
    "## looping through each audio transcript which is generated from assemblyai\n",
    "for audio in audio_transcript:\n",
    "    \n",
    "    ## Removing stop words from the audio transcript using the neattext library functions\n",
    "    cleantext = nfx.remove_stopwords(audio)\n",
    "    ## Removing punctuations from the audio transcript using the neattext library functions\n",
    "    cleantext = nfx.remove_punctuations(cleantext)\n",
    "    ## Removing user handles from the audio transcript using the neattext library functions\n",
    "    cleantext = nfx.remove_userhandles(cleantext)\n",
    "    #3 storing the clean text in cleaned_audiotext list to add it into the dataframe\n",
    "    cleaned_audiotext.append(cleantext)\n",
    "    \n",
    "    #Call to the function get_emotion from text2emotion library which returns the emotion\n",
    "    ## for text in the form of an dictionary for ex: {'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2, 'Sad': 0.2, 'Fear': 0.33}\n",
    "    emotionresult=te.get_emotion(cleantext)\n",
    "    \n",
    "    ## Storing the emotions results for all the audio file in emotions list to add it into the final dataframe\n",
    "    emotions.append(emotionresult)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc225b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I have decided to use Dataframes for those reasons i am importing pandas\n",
    "import pandas as pd\n",
    "\n",
    "## creation of dataframe mydf which include 4 columns \n",
    "    ## audio_filenames : which has mp3 file name\n",
    "    ## audio_transcript : which has audio transcript of the mp3 files we have processed through assembly ai\n",
    "    ## cleaned_audiotext : cleaned audio transcript text result after using the neattext library\n",
    "    ## emotions : which will contain the dictionary result from text2emotion of audio transcript\n",
    "mydf = pd.DataFrame(list(zip(audio_filenames, audio_transcript ,cleaned_audiotext , emotions)), columns = ['File Name', 'Audio Transcript', 'Clean Transcript', 'Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d1def9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Audio Transcript</th>\n",
       "      <th>Clean Transcript</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.\\audio\\Bill_Gates_Chats_with_Ellen_for_the_Fi...</td>\n",
       "      <td>I'm so happy to have you here. This is the fir...</td>\n",
       "      <td>Im happy here time on thanks know nervous entr...</td>\n",
       "      <td>{'Happy': 0.31, 'Angry': 0.03, 'Surprise': 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.\\audio\\Ellen_Taught_This_Fan_How_to_Speak_Eng...</td>\n",
       "      <td>Our next guest is sitting in the audience righ...</td>\n",
       "      <td>guest sitting audience right now you Everybody...</td>\n",
       "      <td>{'Happy': 0.24, 'Angry': 0.2, 'Surprise': 0.23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Str...</td>\n",
       "      <td>Everywhere you go. I'm like, crazy. Well, I me...</td>\n",
       "      <td>go Im like crazy Well meant Id calm down Theyr...</td>\n",
       "      <td>{'Happy': 0.34, 'Angry': 0.03, 'Surprise': 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.\\audio\\Jordan_Peterson___How_to_Have_Better_C...</td>\n",
       "      <td>Exploration. I really thought this was interes...</td>\n",
       "      <td>Exploration thought interesting intellectual d...</td>\n",
       "      <td>{'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.\\audio\\Penn_Badgley_Can_Go_From_Charming_To_C...</td>\n",
       "      <td>Listen, everybody, you know my next guest from...</td>\n",
       "      <td>Listen everybody know guest Gossip Girl easy J...</td>\n",
       "      <td>{'Happy': 0.33, 'Angry': 0.04, 'Surprise': 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.\\audio\\Small_Talk.mp3</td>\n",
       "      <td>Excuse me. Hi. I'm trying to relax. Would you ...</td>\n",
       "      <td>Excuse me Hi Im trying relax mind Oh sorry Mr ...</td>\n",
       "      <td>{'Happy': 0.26, 'Angry': 0.02, 'Surprise': 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.\\audio\\The_Oprah_Conversation_—_Will_Smith_On...</td>\n",
       "      <td>To this day, if we start talking it's 4 hours....</td>\n",
       "      <td>day start talking 4 hours 4 hours exchange sen...</td>\n",
       "      <td>{'Happy': 0.38, 'Angry': 0.04, 'Surprise': 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.\\audio\\The_Viral_Voice_that_Sounds_Like_Siri_...</td>\n",
       "      <td>My next guest blows my mind, and I'm sure she'...</td>\n",
       "      <td>guest blows mind Im sure going you Matter fact...</td>\n",
       "      <td>{'Happy': 0.14, 'Angry': 0.06, 'Surprise': 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File Name  \\\n",
       "0  .\\audio\\Bill_Gates_Chats_with_Ellen_for_the_Fi...   \n",
       "1  .\\audio\\Ellen_Taught_This_Fan_How_to_Speak_Eng...   \n",
       "2  .\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Str...   \n",
       "3  .\\audio\\Jordan_Peterson___How_to_Have_Better_C...   \n",
       "4  .\\audio\\Penn_Badgley_Can_Go_From_Charming_To_C...   \n",
       "5                             .\\audio\\Small_Talk.mp3   \n",
       "6  .\\audio\\The_Oprah_Conversation_—_Will_Smith_On...   \n",
       "7  .\\audio\\The_Viral_Voice_that_Sounds_Like_Siri_...   \n",
       "\n",
       "                                    Audio Transcript  \\\n",
       "0  I'm so happy to have you here. This is the fir...   \n",
       "1  Our next guest is sitting in the audience righ...   \n",
       "2  Everywhere you go. I'm like, crazy. Well, I me...   \n",
       "3  Exploration. I really thought this was interes...   \n",
       "4  Listen, everybody, you know my next guest from...   \n",
       "5  Excuse me. Hi. I'm trying to relax. Would you ...   \n",
       "6  To this day, if we start talking it's 4 hours....   \n",
       "7  My next guest blows my mind, and I'm sure she'...   \n",
       "\n",
       "                                    Clean Transcript  \\\n",
       "0  Im happy here time on thanks know nervous entr...   \n",
       "1  guest sitting audience right now you Everybody...   \n",
       "2  go Im like crazy Well meant Id calm down Theyr...   \n",
       "3  Exploration thought interesting intellectual d...   \n",
       "4  Listen everybody know guest Gossip Girl easy J...   \n",
       "5  Excuse me Hi Im trying relax mind Oh sorry Mr ...   \n",
       "6  day start talking 4 hours 4 hours exchange sen...   \n",
       "7  guest blows mind Im sure going you Matter fact...   \n",
       "\n",
       "                                             Emotion  \n",
       "0  {'Happy': 0.31, 'Angry': 0.03, 'Surprise': 0.2...  \n",
       "1  {'Happy': 0.24, 'Angry': 0.2, 'Surprise': 0.23...  \n",
       "2  {'Happy': 0.34, 'Angry': 0.03, 'Surprise': 0.1...  \n",
       "3  {'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2,...  \n",
       "4  {'Happy': 0.33, 'Angry': 0.04, 'Surprise': 0.2...  \n",
       "5  {'Happy': 0.26, 'Angry': 0.02, 'Surprise': 0.1...  \n",
       "6  {'Happy': 0.38, 'Angry': 0.04, 'Surprise': 0.1...  \n",
       "7  {'Happy': 0.14, 'Angry': 0.06, 'Surprise': 0.2...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a66a37",
   "metadata": {},
   "source": [
    "#### Step 4 : Topic Extraction\n",
    "\n",
    "Firstly I have to find techniques in NLP that can work unsupervised text data after thorough research i came acrros a techniques in NLP called topic modelling.\n",
    "\n",
    "Topic modeling is a technique used in natural language processing (NLP) to discover latent topics or themes in a collection of documents. It is a way to automatically identify the topics present in a large corpus of text data, and to infer the underlying structure that connects these topics to individual documents.\n",
    "\n",
    "There are several popular algorithms for topic modeling, such as Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF). These algorithms work by analyzing the distribution of words across the corpus and identifying patterns that suggest the presence of distinct topics. Once the topics are identified, they can be used for a variety of tasks such as document classification, summarization, and recommendation systems.\n",
    "\n",
    "I have tested Both the methods in the following sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88583b4e",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Two important notes:\n",
    "    --> The user must decide gpf the amount of topics present in the document.\n",
    "    --> The user must interpret what the topics are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a902f680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x987 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1408 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Little bit of preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## count venctorizer\n",
    "## max_df = when building up the vocabulary we are going to ingore the terms which has really high document frequency\n",
    "## 0.9 discard the words which show up in 90 % of the documents\n",
    "\n",
    "## min_df = words that show up minimum number of times. for a word to be counted it is atleast present in one document\n",
    "cv = CountVectorizer(max_df = 0.9, min_df= 1,stop_words ='english')\n",
    "\n",
    "## since it is unsupervised we are going to do fit tranform to the entire dataset\n",
    "dtm = cv.fit_transform(mydf['Audio Transcript'])\n",
    "\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1ddc210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 15 words for Topic #0\n",
      "['nice', 'alexa', 'creepy', 'got', 'said', 'mean', 'light', 'okay', 'kind', 'say', 'people', 'right', 'love', 'did', 'yeah']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #1\n",
      "['trying', 'matter', 've', 'good', 'way', 'life', 'best', 'conversation', 'goodbye', 'pieces', 'sort', 'think', 'let', 'things', 'thought']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #2\n",
      "['thought', 'know', 'interesting', 'maybe', 'better', 'kind', 'kids', 'thing', 'don', 'things', 'yeah', 'conversation', 'think', 'right', 'people']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #3\n",
      "['dictionary', 'learned', 'god', 'know', 'english', 'okay', 'noun', 'affectionate', 'standing', 'stay', 'love', 'ellen', 'watch', 'word', 'right']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #4\n",
      "['trying', 'matter', 've', 'good', 'way', 'life', 'best', 'conversation', 'goodbye', 'pieces', 'sort', 'think', 'let', 'things', 'thought']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #5\n",
      "['trying', 'matter', 've', 'good', 'way', 'life', 'best', 'conversation', 'goodbye', 'pieces', 'sort', 'think', 'let', 'things', 'thought']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #6\n",
      "['trying', 'matter', 've', 'good', 'way', 'life', 'best', 'conversation', 'goodbye', 'pieces', 'sort', 'think', 'let', 'things', 'thought']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #7\n",
      "['listen', 'girl', 'hey', 'adam', 'elizabeth', 'left', 'goodbye', 'kid', 'girlfriends', 'grumpy', 'old', 'gone', 'katie', 'girlfriend', 'mr']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #8\n",
      "['doing', 'long', 've', 'broken', 'yeah', 'people', 'freedom', 'love', 'figure', 'make', 'right', 'actually', 'come', 'going', 'happy']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #9\n",
      "['trying', 'matter', 've', 'good', 'way', 'life', 'best', 'conversation', 'goodbye', 'pieces', 'sort', 'think', 'let', 'things', 'thought']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "## There are many propeties in LDA here we are only using \n",
    "## n_components = Increase the number if you want more subtopics in each topic\n",
    "LDA = LatentDirichletAllocation(n_components = 10,random_state = 42)\n",
    "\n",
    "## Fit it to Document Term Matrix\n",
    "LDA.fit(dtm)\n",
    "\n",
    "## Grabbing the top 15 words from each topic\n",
    "# argsort does is return the index positions that would sort the array least to most\n",
    "for i, topic in enumerate(LDA.components_):\n",
    "    print(f\"The top 15 words for Topic #{i}\")\n",
    "    print([cv.get_feature_names()[index] for index in topic.argsort()[-15:]])\n",
    "    print('\\n')\n",
    "    \n",
    "## Attach the topics to audio transcripts  \n",
    "topic_results= LDA.transform(dtm)\n",
    "\n",
    "## Assign the audio transcript high probability topics\n",
    "mydf['Topic'] = topic_results.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5888de22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Audio Transcript</th>\n",
       "      <th>Clean Transcript</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.\\audio\\Bill_Gates_Chats_with_Ellen_for_the_Fi...</td>\n",
       "      <td>I'm so happy to have you here. This is the fir...</td>\n",
       "      <td>Im happy here time on thanks know nervous entr...</td>\n",
       "      <td>{'Happy': 0.31, 'Angry': 0.03, 'Surprise': 0.2...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.\\audio\\Ellen_Taught_This_Fan_How_to_Speak_Eng...</td>\n",
       "      <td>Our next guest is sitting in the audience righ...</td>\n",
       "      <td>guest sitting audience right now you Everybody...</td>\n",
       "      <td>{'Happy': 0.24, 'Angry': 0.2, 'Surprise': 0.23...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Str...</td>\n",
       "      <td>Everywhere you go. I'm like, crazy. Well, I me...</td>\n",
       "      <td>go Im like crazy Well meant Id calm down Theyr...</td>\n",
       "      <td>{'Happy': 0.34, 'Angry': 0.03, 'Surprise': 0.1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.\\audio\\Jordan_Peterson___How_to_Have_Better_C...</td>\n",
       "      <td>Exploration. I really thought this was interes...</td>\n",
       "      <td>Exploration thought interesting intellectual d...</td>\n",
       "      <td>{'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.\\audio\\Penn_Badgley_Can_Go_From_Charming_To_C...</td>\n",
       "      <td>Listen, everybody, you know my next guest from...</td>\n",
       "      <td>Listen everybody know guest Gossip Girl easy J...</td>\n",
       "      <td>{'Happy': 0.33, 'Angry': 0.04, 'Surprise': 0.2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.\\audio\\Small_Talk.mp3</td>\n",
       "      <td>Excuse me. Hi. I'm trying to relax. Would you ...</td>\n",
       "      <td>Excuse me Hi Im trying relax mind Oh sorry Mr ...</td>\n",
       "      <td>{'Happy': 0.26, 'Angry': 0.02, 'Surprise': 0.1...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.\\audio\\The_Oprah_Conversation_—_Will_Smith_On...</td>\n",
       "      <td>To this day, if we start talking it's 4 hours....</td>\n",
       "      <td>day start talking 4 hours 4 hours exchange sen...</td>\n",
       "      <td>{'Happy': 0.38, 'Angry': 0.04, 'Surprise': 0.1...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.\\audio\\The_Viral_Voice_that_Sounds_Like_Siri_...</td>\n",
       "      <td>My next guest blows my mind, and I'm sure she'...</td>\n",
       "      <td>guest blows mind Im sure going you Matter fact...</td>\n",
       "      <td>{'Happy': 0.14, 'Angry': 0.06, 'Surprise': 0.2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File Name  \\\n",
       "0  .\\audio\\Bill_Gates_Chats_with_Ellen_for_the_Fi...   \n",
       "1  .\\audio\\Ellen_Taught_This_Fan_How_to_Speak_Eng...   \n",
       "2  .\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Str...   \n",
       "3  .\\audio\\Jordan_Peterson___How_to_Have_Better_C...   \n",
       "4  .\\audio\\Penn_Badgley_Can_Go_From_Charming_To_C...   \n",
       "5                             .\\audio\\Small_Talk.mp3   \n",
       "6  .\\audio\\The_Oprah_Conversation_—_Will_Smith_On...   \n",
       "7  .\\audio\\The_Viral_Voice_that_Sounds_Like_Siri_...   \n",
       "\n",
       "                                    Audio Transcript  \\\n",
       "0  I'm so happy to have you here. This is the fir...   \n",
       "1  Our next guest is sitting in the audience righ...   \n",
       "2  Everywhere you go. I'm like, crazy. Well, I me...   \n",
       "3  Exploration. I really thought this was interes...   \n",
       "4  Listen, everybody, you know my next guest from...   \n",
       "5  Excuse me. Hi. I'm trying to relax. Would you ...   \n",
       "6  To this day, if we start talking it's 4 hours....   \n",
       "7  My next guest blows my mind, and I'm sure she'...   \n",
       "\n",
       "                                    Clean Transcript  \\\n",
       "0  Im happy here time on thanks know nervous entr...   \n",
       "1  guest sitting audience right now you Everybody...   \n",
       "2  go Im like crazy Well meant Id calm down Theyr...   \n",
       "3  Exploration thought interesting intellectual d...   \n",
       "4  Listen everybody know guest Gossip Girl easy J...   \n",
       "5  Excuse me Hi Im trying relax mind Oh sorry Mr ...   \n",
       "6  day start talking 4 hours 4 hours exchange sen...   \n",
       "7  guest blows mind Im sure going you Matter fact...   \n",
       "\n",
       "                                             Emotion  Topic  \n",
       "0  {'Happy': 0.31, 'Angry': 0.03, 'Surprise': 0.2...      2  \n",
       "1  {'Happy': 0.24, 'Angry': 0.2, 'Surprise': 0.23...      3  \n",
       "2  {'Happy': 0.34, 'Angry': 0.03, 'Surprise': 0.1...      0  \n",
       "3  {'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2,...      2  \n",
       "4  {'Happy': 0.33, 'Angry': 0.04, 'Surprise': 0.2...      0  \n",
       "5  {'Happy': 0.26, 'Angry': 0.02, 'Surprise': 0.1...      7  \n",
       "6  {'Happy': 0.38, 'Angry': 0.04, 'Surprise': 0.1...      8  \n",
       "7  {'Happy': 0.14, 'Angry': 0.06, 'Surprise': 0.2...      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301eb7f0",
   "metadata": {},
   "source": [
    "\n",
    " #### Non Negative Matrix Factorization   \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " #### https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d575f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76e6a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df = 0.9, min_df= 1,stop_words ='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74408278",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(mydf['Clean Transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8be0675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x991 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1393 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6babacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd37307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF (n_components = 10, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8c275e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(n_components=10, random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49117e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 15 words for Topic #0\n",
      "['love', 'videos', 'hug', 'stuck', 'dictionary', 'dominican', 'english', 'stay', 'watch', 'affectionate', 'standing', 'noun', 'ellen', 'right', 'word']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #1\n",
      "['40', 'school', 'teachers', 'billion', 'porsche', 'terms', 'help', 'people', 'huge', 'thing', 'billionaire', 'money', 'right', 'yeah', 'kids']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #2\n",
      "['line', 'hood', 'recorded', 'game', 'cutting', 'mad', 'professional', 'hey', 'going', 'said', 'video', 'siri', 'cut', 'alexa', 'light']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #3\n",
      "['girl', 'looks', 'character', 'fun', 'remember', 'weird', 'called', 'whos', 'listen', 'love', 'okay', 'mean', 'right', 'people', 'yeah']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #4\n",
      "['left', 'cried', 'brought', 'laughing', 'wheres', 'flower', 'boring', 'kid', 'old', 'katie', 'grumpy', 'girlfriends', 'gone', 'girlfriend', 'mr']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #5\n",
      "['sort', 'know', 'lot', 'means', 'youtube', 'engaged', 'people', 'theres', 'conversations', 'things', 'interesting', 'better', 'think', 'maybe', 'conversation']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #6\n",
      "['self', '100', 'separated', 'cup', 'relationship', 'agreed', 'versus', 'right', 'come', 'freedom', 'actually', 'going', 'broken', 'figure', 'happy']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #7\n",
      "['say', 'penn', 'wow', 'bed', 'man', 'hey', 'turn', 'welcome', 'meet', 'charming', 'want', 'camera', 'joe', 'creepy', 'nice']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #8\n",
      "['story', 'year', 'lot', 'based', 'brilliant', 'guess', 'accent', 'wrote', 'called', 'kind', 'love', 'sounds', 'job', 'theyre', 'yeah']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #9\n",
      "['shirt', 'fully', 'clothed', 'portrait', 'circumstance', 'wow', 'say', 'meet', 'bed', 'penn', 'charming', 'nice', 'joe', 'camera', 'creepy']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(nmf_model.components_):\n",
    "    print(f\"The top 15 words for Topic #{i}\")\n",
    "    print([tfidf.get_feature_names()[index] for index in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dcb542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = nmf_model.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "105b5cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 8, 5, 9, 4, 6, 2], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results.argmax(axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f72f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf['Topic'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ceeba4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Audio Transcript</th>\n",
       "      <th>Clean Transcript</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.\\audio\\Bill_Gates_Chats_with_Ellen_for_the_Fi...</td>\n",
       "      <td>I'm so happy to have you here. This is the fir...</td>\n",
       "      <td>Im happy here time on thanks know nervous entr...</td>\n",
       "      <td>{'Happy': 0.31, 'Angry': 0.03, 'Surprise': 0.2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.\\audio\\Ellen_Taught_This_Fan_How_to_Speak_Eng...</td>\n",
       "      <td>Our next guest is sitting in the audience righ...</td>\n",
       "      <td>guest sitting audience right now you Everybody...</td>\n",
       "      <td>{'Happy': 0.24, 'Angry': 0.2, 'Surprise': 0.23...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Str...</td>\n",
       "      <td>Everywhere you go. I'm like, crazy. Well, I me...</td>\n",
       "      <td>go Im like crazy Well meant Id calm down Theyr...</td>\n",
       "      <td>{'Happy': 0.34, 'Angry': 0.03, 'Surprise': 0.1...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.\\audio\\Jordan_Peterson___How_to_Have_Better_C...</td>\n",
       "      <td>Exploration. I really thought this was interes...</td>\n",
       "      <td>Exploration thought interesting intellectual d...</td>\n",
       "      <td>{'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2,...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.\\audio\\Penn_Badgley_Can_Go_From_Charming_To_C...</td>\n",
       "      <td>Listen, everybody, you know my next guest from...</td>\n",
       "      <td>Listen everybody know guest Gossip Girl easy J...</td>\n",
       "      <td>{'Happy': 0.33, 'Angry': 0.04, 'Surprise': 0.2...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.\\audio\\Small_Talk.mp3</td>\n",
       "      <td>Excuse me. Hi. I'm trying to relax. Would you ...</td>\n",
       "      <td>Excuse me Hi Im trying relax mind Oh sorry Mr ...</td>\n",
       "      <td>{'Happy': 0.26, 'Angry': 0.02, 'Surprise': 0.1...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.\\audio\\The_Oprah_Conversation_—_Will_Smith_On...</td>\n",
       "      <td>To this day, if we start talking it's 4 hours....</td>\n",
       "      <td>day start talking 4 hours 4 hours exchange sen...</td>\n",
       "      <td>{'Happy': 0.38, 'Angry': 0.04, 'Surprise': 0.1...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.\\audio\\The_Viral_Voice_that_Sounds_Like_Siri_...</td>\n",
       "      <td>My next guest blows my mind, and I'm sure she'...</td>\n",
       "      <td>guest blows mind Im sure going you Matter fact...</td>\n",
       "      <td>{'Happy': 0.14, 'Angry': 0.06, 'Surprise': 0.2...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File Name  \\\n",
       "0  .\\audio\\Bill_Gates_Chats_with_Ellen_for_the_Fi...   \n",
       "1  .\\audio\\Ellen_Taught_This_Fan_How_to_Speak_Eng...   \n",
       "2  .\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Str...   \n",
       "3  .\\audio\\Jordan_Peterson___How_to_Have_Better_C...   \n",
       "4  .\\audio\\Penn_Badgley_Can_Go_From_Charming_To_C...   \n",
       "5                             .\\audio\\Small_Talk.mp3   \n",
       "6  .\\audio\\The_Oprah_Conversation_—_Will_Smith_On...   \n",
       "7  .\\audio\\The_Viral_Voice_that_Sounds_Like_Siri_...   \n",
       "\n",
       "                                    Audio Transcript  \\\n",
       "0  I'm so happy to have you here. This is the fir...   \n",
       "1  Our next guest is sitting in the audience righ...   \n",
       "2  Everywhere you go. I'm like, crazy. Well, I me...   \n",
       "3  Exploration. I really thought this was interes...   \n",
       "4  Listen, everybody, you know my next guest from...   \n",
       "5  Excuse me. Hi. I'm trying to relax. Would you ...   \n",
       "6  To this day, if we start talking it's 4 hours....   \n",
       "7  My next guest blows my mind, and I'm sure she'...   \n",
       "\n",
       "                                    Clean Transcript  \\\n",
       "0  Im happy here time on thanks know nervous entr...   \n",
       "1  guest sitting audience right now you Everybody...   \n",
       "2  go Im like crazy Well meant Id calm down Theyr...   \n",
       "3  Exploration thought interesting intellectual d...   \n",
       "4  Listen everybody know guest Gossip Girl easy J...   \n",
       "5  Excuse me Hi Im trying relax mind Oh sorry Mr ...   \n",
       "6  day start talking 4 hours 4 hours exchange sen...   \n",
       "7  guest blows mind Im sure going you Matter fact...   \n",
       "\n",
       "                                             Emotion  Topic  \n",
       "0  {'Happy': 0.31, 'Angry': 0.03, 'Surprise': 0.2...      1  \n",
       "1  {'Happy': 0.24, 'Angry': 0.2, 'Surprise': 0.23...      0  \n",
       "2  {'Happy': 0.34, 'Angry': 0.03, 'Surprise': 0.1...      8  \n",
       "3  {'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2,...      5  \n",
       "4  {'Happy': 0.33, 'Angry': 0.04, 'Surprise': 0.2...      9  \n",
       "5  {'Happy': 0.26, 'Angry': 0.02, 'Surprise': 0.1...      4  \n",
       "6  {'Happy': 0.38, 'Angry': 0.04, 'Surprise': 0.1...      6  \n",
       "7  {'Happy': 0.14, 'Angry': 0.06, 'Surprise': 0.2...      2  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe08585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From the above observation \n",
    "## NMF works better for the data i am working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ba6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
