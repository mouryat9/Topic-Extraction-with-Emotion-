{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06373fb6",
   "metadata": {},
   "source": [
    "# Speech Analysis in conversation in computer science Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9605ef01",
   "metadata": {},
   "source": [
    "### Initial Plan of Attack :\n",
    "    Step 1 : Audio to Text File Conversion\n",
    "    Step 2 : Cleaning the data\n",
    "    Step 3 : Emotion Detection\n",
    "    Step 4 : Topic Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc62bc1",
   "metadata": {},
   "source": [
    "#### Firstly Lets Just Install the libraries Required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303823c",
   "metadata": {},
   "source": [
    "Text2Emotion is a python package designed to identify emotions in text data. It works by recognizing emotions expressed in words when people are confident in their statements. For example, a dissatisfied customer may say, \"I am very angry by your product services and gonna file a complaint.\" Text2Emotion can extract emotions from text and categorize them as Happy, Angry, Sad, Surprise, or Fear, providing a dictionary output.\n",
    "\n",
    "#### https://pypi.org/project/text2emotion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84aecd07",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: text2emotion in c:\\users\\mourya kunuku\\appdata\\roaming\\python\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: nltk in c:\\anaconda\\lib\\site-packages (from text2emotion) (3.7)\n",
      "Requirement already satisfied: emoji>=0.6.0 in c:\\users\\mourya kunuku\\appdata\\roaming\\python\\python39\\site-packages (from text2emotion) (1.7.0)\n",
      "Requirement already satisfied: click in c:\\anaconda\\lib\\site-packages (from nltk->text2emotion) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\anaconda\\lib\\site-packages (from nltk->text2emotion) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\anaconda\\lib\\site-packages (from nltk->text2emotion) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from nltk->text2emotion) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mourya kunuku\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk->text2emotion) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install text2emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb88814",
   "metadata": {},
   "source": [
    "NeatText is a simple NLP package for cleaning textual data and text preprocessing. Simplifying Text Cleaning For NLP & ML\n",
    "\n",
    "#### https://pypi.org/project/neattext/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ddab10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting neattext\n",
      "  Downloading neattext-0.1.3-py3-none-any.whl (114 kB)\n",
      "     -------------------------------------- 114.7/114.7 kB 7.0 MB/s eta 0:00:00\n",
      "Installing collected packages: neattext\n",
      "Successfully installed neattext-0.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install neattext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0dde6",
   "metadata": {},
   "source": [
    "#### Please find all the TV show conversations links i used as audio files in the project\n",
    "\n",
    "##### https://www.youtube.com/watch?v=mNIXRXikYDc&ab_channel=TheEllenShow\n",
    "##### https://www.youtube.com/watch?v=hVd_rdhKTVk&ab_channel=AppleTV\n",
    "##### https://www.youtube.com/watch?v=skj-ALA1HFE&ab_channel=VideoAdvice\n",
    "##### https://www.youtube.com/watch?v=PNTCM7cbrsc&t=184s&ab_channel=CampusMovieFest\n",
    "##### https://www.youtube.com/watch?v=uQDDGriA1lk&ab_channel=SteveTVShow\n",
    "##### https://www.youtube.com/watch?v=f5NJQiY9AuY&t=12s&ab_channel=TheEllenShow\n",
    "##### https://www.youtube.com/watch?v=sd7dSHU4BKs&t=12s&ab_channel=Simulation\n",
    "##### https://www.youtube.com/watch?v=yRQ5ntxnFaI&t=2s&ab_channel=TheLateShowwithStephenColbert\n",
    "##### https://www.youtube.com/watch?v=pKtNyN53B_s&t=2s&ab_channel=TheKellyClarksonShow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bba08f",
   "metadata": {},
   "source": [
    "####                                                            Step 1 : Audio to Text File Conversion\n",
    "\n",
    "This code is a Python script that uses the AssemblyAI API to upload an audio file, transcribe it, and return the text transcript.\n",
    "\n",
    "The script uses the requests library to make HTTP requests to the API, and defines three functions:\n",
    "\n",
    "    uploadMyFile to upload the audio file to AssemblyAI and return an upload URL.\n",
    "    startTranscription to initiate the transcription process using the upload URL.\n",
    "    getTranscription to check the status of the transcription and retrieve the text transcript once it is completed.\n",
    "    \n",
    "The script requires an authentication key to use the AssemblyAI API, which must be obtained from the AssemblyAI website. The key is stored in the authKey variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a670b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make requests to the API\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Step 1\n",
    "# Register and get auth key from https://www.assemblyai.com/\n",
    "authKey = 'f9d89818e1214415b6dd6c9642439a6a'\n",
    "\n",
    "# Parameters for HTTP request\n",
    "headers = {\n",
    "    'authorization' : authKey,\n",
    "    'content-type'  : 'application/json'\n",
    "}\n",
    "\n",
    "# Url's for Upload and transcripts assigned by assemblyai\n",
    "uploadUrl      = 'https://api.assemblyai.com/v2/upload'\n",
    "transcriptUrl  = 'https://api.assemblyai.com/v2/transcript'\n",
    "\n",
    "# Step 2 : Upload audio file\n",
    "def uploadMyFile(fileName):\n",
    "\n",
    "    def _readMyFile(fn):\n",
    "        # \n",
    "        chunkSize = 10#5242880\n",
    "        \n",
    "        # Creation of Filestream to Read the file\n",
    "        with open(fn, 'rb') as fileStream:\n",
    "\n",
    "            while True:\n",
    "                data = fileStream.read(chunkSize)\n",
    "                ## Since it is a while we need to end the loop when data is not present\n",
    "                if not data:\n",
    "                    break\n",
    "                ##  we use yeild instead of return for returning the whole file \n",
    "                ##  not just the first chunk of the file becasue we are reading the file in chunks\n",
    "                yield data\n",
    "    ## POST method\n",
    "    response = requests.post(\n",
    "        uploadUrl,\n",
    "        headers= headers,\n",
    "        data= _readMyFile(fileName)\n",
    "    )\n",
    "    \n",
    "    ## every response has a json so we are intializing it to json\n",
    "    json = response.json()\n",
    "\n",
    "    return json['upload_url']\n",
    "# END def uploadMyFile\n",
    "\n",
    "# Step 3 : Start Transcription\n",
    "## we provide the uploaded audio url and expect transcription Id from assemblyai\n",
    "def startTranscription(aurl):\n",
    "    ## POST method\n",
    "    response = requests.post(\n",
    "        transcriptUrl,\n",
    "        headers= headers,\n",
    "        json= { 'audio_url' : aurl }\n",
    "    )\n",
    "    ## every response has a json so we are intializing it to json\n",
    "    json = response.json()\n",
    "\n",
    "    return json['id']\n",
    "# END def startTranscription\n",
    "\n",
    "\n",
    "\n",
    "# Step 4 : Start Transcription\n",
    "## we provide the transcription Id and expect text from assemblyai\n",
    "def getTranscription(tid):\n",
    "\n",
    "    maxAttempts = 1000\n",
    "    timedout    = False\n",
    "\n",
    "    while True:\n",
    "        ## Get method\n",
    "        response = requests.get(\n",
    "            f'{transcriptUrl}/{tid}', #transcriptUrl + '/' + tid,\n",
    "            headers= headers\n",
    "        )\n",
    "        \n",
    "        ## every response has a json so we are intializing it to json\n",
    "        json = response.json()\n",
    "\n",
    "        if json['status'] == 'completed':\n",
    "            break\n",
    "\n",
    "        maxAttempts -= 1\n",
    "        timedout = maxAttempts <= 0\n",
    "\n",
    "        if timedout:\n",
    "            break\n",
    "\n",
    "        # Wait for 3 seconds before make the next try!\n",
    "        # Why? Because we don't want to set AssemblyAI on Fire!!!\n",
    "        # Search at youtube for: IT Crowd Fire at a Sea Parks\n",
    "        time.sleep(3)\n",
    "\n",
    "    return 'Timeout...' if timedout else json['text']\n",
    "# END def getTranscription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f4f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filenames = []\n",
    "audio_transcript = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "415670ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mourya\n",
      "[nltk_data]     Kunuku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Mourya\n",
      "[nltk_data]     Kunuku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mourya\n",
      "[nltk_data]     Kunuku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import the modules\n",
    "import text2emotion as te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c5500a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing : .\\audio\\Bill_Gates_Chats_with_Ellen_for_the_First_Time.mp3\n",
      "processing Completed: .\\audio\\Bill_Gates_Chats_with_Ellen_for_the_First_Time.mp3\n",
      "processing : .\\audio\\Ellen_Taught_This_Fan_How_to_Speak_English.mp3\n",
      "processing Completed: .\\audio\\Ellen_Taught_This_Fan_How_to_Speak_English.mp3\n",
      "processing : .\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Strange'_Intimate_Scenes_In_'Conversations_With_Friends'.mp3\n",
      "processing Completed: .\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Strange'_Intimate_Scenes_In_'Conversations_With_Friends'.mp3\n",
      "processing : .\\audio\\Jordan_Peterson___How_to_Have_Better_Conversations.mp3\n",
      "processing Completed: .\\audio\\Jordan_Peterson___How_to_Have_Better_Conversations.mp3\n",
      "processing : .\\audio\\Penn_Badgley_Can_Go_From_Charming_To_Creepy_Without_Changing_His_Expression.mp3\n",
      "processing Completed: .\\audio\\Penn_Badgley_Can_Go_From_Charming_To_Creepy_Without_Changing_His_Expression.mp3\n",
      "processing : .\\audio\\Small_Talk.mp3\n",
      "processing Completed: .\\audio\\Small_Talk.mp3\n",
      "processing : .\\audio\\The_Oprah_Conversation_—_Will_Smith_On_His_Marriage_to_Jada___Apple_TV+.mp3\n",
      "processing Completed: .\\audio\\The_Oprah_Conversation_—_Will_Smith_On_His_Marriage_to_Jada___Apple_TV+.mp3\n",
      "processing : .\\audio\\The_Viral_Voice_that_Sounds_Like_Siri____STEVE_HARVEY.mp3\n",
      "processing Completed: .\\audio\\The_Viral_Voice_that_Sounds_Like_Siri____STEVE_HARVEY.mp3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def mp3gen():\n",
    "    for root, dirs, files in os.walk('.'):\n",
    "        for filename in files:\n",
    "            if os.path.splitext(filename)[1] == \".mp3\":\n",
    "                yield os.path.join(root, filename)\n",
    "\n",
    "for mp3file in mp3gen():\n",
    "#     print(f\"r{mp3file}\")\n",
    "    print(f\"processing : {mp3file}\")\n",
    "    audioUrl = uploadMyFile(mp3file)\n",
    "\n",
    "    # step 2) Start Transcription\n",
    "    transcriptionID = startTranscription(audioUrl)\n",
    "\n",
    "    # step 3) Get Transcription Text\n",
    "    text = getTranscription(transcriptionID)\n",
    "    \n",
    "    \n",
    "    audio_filenames.append(mp3file)\n",
    "    audio_transcript.append(text)\n",
    "    \n",
    "    print(f\"processing Completed: {mp3file}\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "feed6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neattext.functions as nfx\n",
    "## Emotions list to store all the emotion results from each audio\n",
    "emotions = []\n",
    "##  To store the cleaned text record for each audio\n",
    "cleaned_audiotext = []\n",
    "\n",
    "## looping through each audio transcript which is generated from assemblyai\n",
    "for audio in audio_transcript:\n",
    "    \n",
    "    ## Removing stop words from the audio transcript using the neattext library functions\n",
    "    cleantext = nfx.remove_stopwords(audio)\n",
    "    ## Removing punctuations from the audio transcript using the neattext library functions\n",
    "    cleantext = nfx.remove_punctuations(cleantext)\n",
    "    ## Removing user handles from the audio transcript using the neattext library functions\n",
    "    cleantext = nfx.remove_userhandles(cleantext)\n",
    "    #3 storing the clean text in cleaned_audiotext list to add it into the dataframe\n",
    "    cleaned_audiotext.append(cleantext)\n",
    "    \n",
    "    #Call to the function get_emotion from text2emotion library which returns the emotion\n",
    "    ## for text in the form of an dictionary for ex: {'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2, 'Sad': 0.2, 'Fear': 0.33}\n",
    "    emotionresult=te.get_emotion(cleantext)\n",
    "    \n",
    "    ## Storing the emotions results for all the audio file in emotions list to add it into the final dataframe\n",
    "    emotions.append(emotionresult)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5807e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mydf = pd.DataFrame(list(zip(audio_filenames, audio_transcript ,cleaned_audiotext , emotions)), columns = ['File Name', 'Audio Transcript', 'Clean Transcript', 'Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1a8d05f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Audio Transcript</th>\n",
       "      <th>Clean Transcript</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.\\audio\\Bill_Gates_Chats_with_Ellen_for_the_Fi...</td>\n",
       "      <td>I'm so happy to have you here. This is the fir...</td>\n",
       "      <td>Im happy here time on thanks know nervous entr...</td>\n",
       "      <td>{'Happy': 0.31, 'Angry': 0.03, 'Surprise': 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.\\audio\\Ellen_Taught_This_Fan_How_to_Speak_Eng...</td>\n",
       "      <td>Our next guest is sitting in the audience righ...</td>\n",
       "      <td>guest sitting audience right now you Everybody...</td>\n",
       "      <td>{'Happy': 0.24, 'Angry': 0.2, 'Surprise': 0.23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Str...</td>\n",
       "      <td>Everywhere you go. I'm like, crazy. Well, I me...</td>\n",
       "      <td>go Im like crazy Well meant Id calm down Theyr...</td>\n",
       "      <td>{'Happy': 0.34, 'Angry': 0.03, 'Surprise': 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.\\audio\\Jordan_Peterson___How_to_Have_Better_C...</td>\n",
       "      <td>Exploration. I really thought this was interes...</td>\n",
       "      <td>Exploration thought interesting intellectual d...</td>\n",
       "      <td>{'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.\\audio\\Penn_Badgley_Can_Go_From_Charming_To_C...</td>\n",
       "      <td>Listen, everybody, you know my next guest from...</td>\n",
       "      <td>Listen everybody know guest Gossip Girl easy J...</td>\n",
       "      <td>{'Happy': 0.33, 'Angry': 0.04, 'Surprise': 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.\\audio\\Small_Talk.mp3</td>\n",
       "      <td>Excuse me. Hi. I'm trying to relax. Would you ...</td>\n",
       "      <td>Excuse me Hi Im trying relax mind Oh sorry Mr ...</td>\n",
       "      <td>{'Happy': 0.26, 'Angry': 0.02, 'Surprise': 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.\\audio\\The_Oprah_Conversation_—_Will_Smith_On...</td>\n",
       "      <td>To this day, if we start talking it's 4 hours....</td>\n",
       "      <td>day start talking 4 hours 4 hours exchange sen...</td>\n",
       "      <td>{'Happy': 0.38, 'Angry': 0.04, 'Surprise': 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.\\audio\\The_Viral_Voice_that_Sounds_Like_Siri_...</td>\n",
       "      <td>My next guest blows my mind, and I'm sure she'...</td>\n",
       "      <td>guest blows mind Im sure going you Matter fact...</td>\n",
       "      <td>{'Happy': 0.14, 'Angry': 0.06, 'Surprise': 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File Name  \\\n",
       "0  .\\audio\\Bill_Gates_Chats_with_Ellen_for_the_Fi...   \n",
       "1  .\\audio\\Ellen_Taught_This_Fan_How_to_Speak_Eng...   \n",
       "2  .\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Str...   \n",
       "3  .\\audio\\Jordan_Peterson___How_to_Have_Better_C...   \n",
       "4  .\\audio\\Penn_Badgley_Can_Go_From_Charming_To_C...   \n",
       "5                             .\\audio\\Small_Talk.mp3   \n",
       "6  .\\audio\\The_Oprah_Conversation_—_Will_Smith_On...   \n",
       "7  .\\audio\\The_Viral_Voice_that_Sounds_Like_Siri_...   \n",
       "\n",
       "                                    Audio Transcript  \\\n",
       "0  I'm so happy to have you here. This is the fir...   \n",
       "1  Our next guest is sitting in the audience righ...   \n",
       "2  Everywhere you go. I'm like, crazy. Well, I me...   \n",
       "3  Exploration. I really thought this was interes...   \n",
       "4  Listen, everybody, you know my next guest from...   \n",
       "5  Excuse me. Hi. I'm trying to relax. Would you ...   \n",
       "6  To this day, if we start talking it's 4 hours....   \n",
       "7  My next guest blows my mind, and I'm sure she'...   \n",
       "\n",
       "                                    Clean Transcript  \\\n",
       "0  Im happy here time on thanks know nervous entr...   \n",
       "1  guest sitting audience right now you Everybody...   \n",
       "2  go Im like crazy Well meant Id calm down Theyr...   \n",
       "3  Exploration thought interesting intellectual d...   \n",
       "4  Listen everybody know guest Gossip Girl easy J...   \n",
       "5  Excuse me Hi Im trying relax mind Oh sorry Mr ...   \n",
       "6  day start talking 4 hours 4 hours exchange sen...   \n",
       "7  guest blows mind Im sure going you Matter fact...   \n",
       "\n",
       "                                             Emotion  \n",
       "0  {'Happy': 0.31, 'Angry': 0.03, 'Surprise': 0.2...  \n",
       "1  {'Happy': 0.24, 'Angry': 0.2, 'Surprise': 0.23...  \n",
       "2  {'Happy': 0.34, 'Angry': 0.03, 'Surprise': 0.1...  \n",
       "3  {'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2,...  \n",
       "4  {'Happy': 0.33, 'Angry': 0.04, 'Surprise': 0.2...  \n",
       "5  {'Happy': 0.26, 'Angry': 0.02, 'Surprise': 0.1...  \n",
       "6  {'Happy': 0.38, 'Angry': 0.04, 'Surprise': 0.1...  \n",
       "7  {'Happy': 0.14, 'Angry': 0.06, 'Surprise': 0.2...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ee4d4",
   "metadata": {},
   "source": [
    "##### VADER sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa4b9e",
   "metadata": {},
   "source": [
    "##### latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "46dab80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "de570514",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df = 0.9, min_df= 2,stop_words ='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "63514ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = cv.fit_transform(mydf['Clean Transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84dc973c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x221 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 623 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ebc44193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3949446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LatentDirichletAllocation(n_components = 7,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a0cde52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=7, random_state=42)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2dec0d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3d8f90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5ba8c76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LDA.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "576459b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_topic = LDA.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8f0f47f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([101,  13, 137, 122,  17,  77, 113,  98, 125,   9, 199, 210,  80,\n",
       "         0, 178, 153,  39, 198, 103, 192, 143, 193,  88, 148,   2, 173,\n",
       "       163, 109, 142, 119, 146,  20, 205, 123,  33, 124, 121, 149,  82,\n",
       "        60, 144, 164, 217, 172, 134,  25,  91,  53,  85,  27,  90, 102,\n",
       "       128,   1,   7, 112,  58, 127,  57, 152, 155, 132,  15,  70,  14,\n",
       "         6,  11,  89, 183, 129,  46,  47, 133, 170, 214,  81, 179, 154,\n",
       "       185, 168, 136,  52,  48, 211, 220,  51,  79,   8, 117,  43,  22,\n",
       "       105,   4, 126, 139,  35,  41, 175,  16,  68, 135,  45, 140,  32,\n",
       "       106,   5, 206, 196, 194,  67, 147,  74,  96,  92, 114,  44, 189,\n",
       "       180, 200,  19, 145,  49,   3,  38, 169,  69, 115,  71, 190,  75,\n",
       "       150, 207, 188, 177, 213,  26, 212, 208, 182, 218, 110, 184,  61,\n",
       "       100,  86, 195, 138, 187, 191, 186,  73,  24,  62,  63,  12, 118,\n",
       "       167, 159, 201, 166,  37, 176, 116, 181, 215, 108,  34, 174, 120,\n",
       "        99, 151, 209, 219,  65,  84,  28,  36, 204,  50, 162,  18, 161,\n",
       "        87,  55, 158,  30,  66,  78,  83, 165,  56,  23, 141, 216,  64,\n",
       "       157,  21,  94, 197,  72,  10, 130,  54, 104,  76,  29,  42, 107,\n",
       "        97,  59, 160,  93,  31, 203, 171,  40,  95, 131, 111, 202, 156],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_topic.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95757519",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_words= single_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "faf4701d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind\n",
      "day\n",
      "watching\n",
      "stay\n",
      "english\n",
      "know\n",
      "okay\n",
      "love\n",
      "watch\n",
      "right\n"
     ]
    }
   ],
   "source": [
    "for index in top_ten_words:\n",
    "    print(cv.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfdf336b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 15 words for Topic #0\n",
      "['hi', 'happy', 'll', 'getting', 'simple', 'possible', 'goes', 'literally', 'welcome', 'turn', 'floor', 'yes', 'wait', 'came', 'guest']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #1\n",
      "['hi', 'happy', 'll', 'getting', 'simple', 'possible', 'goes', 'literally', 'welcome', 'turn', 'floor', 'yes', 'wait', 'came', 'guest']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #2\n",
      "['ve', 'english', 'oh', 'look', 'watching', 'stay', 'know', 'got', 'day', 'kind', 'okay', 'love', 'watch', 'girlfriend', 'right']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #3\n",
      "['meet', 'lot', 'time', 'thing', 'kids', 'okay', 'mean', 'don', 'say', 'love', 'kind', 'did', 'people', 'right', 'yeah']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #4\n",
      "['hi', 'happy', 'll', 'getting', 'simple', 'possible', 'goes', 'literally', 'welcome', 'turn', 'floor', 'yes', 'wait', 'came', 'guest']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #5\n",
      "['hi', 'happy', 'll', 'getting', 'simple', 'possible', 'goes', 'literally', 'welcome', 'turn', 'floor', 'yes', 'wait', 'came', 'guest']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #6\n",
      "['better', 'interesting', 'know', 'happy', 've', 'actually', 'come', 'make', 'things', 'think', 'light', 'people', 'right', 'conversation', 'going']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(LDA.components_):\n",
    "    print(f\"The top 15 words for Topic #{i}\")\n",
    "    print([cv.get_feature_names()[index] for index in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d9391aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results= LDA.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a56d705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 7)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1f0ed0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf['Topic'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8ec5765b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Audio Transcript</th>\n",
       "      <th>Clean Transcript</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.\\audio\\Bill_Gates_Chats_with_Ellen_for_the_Fi...</td>\n",
       "      <td>I'm so happy to have you here. This is the fir...</td>\n",
       "      <td>Im happy here time on thanks know nervous entr...</td>\n",
       "      <td>{'Happy': 0.31, 'Angry': 0.03, 'Surprise': 0.2...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.\\audio\\Ellen_Taught_This_Fan_How_to_Speak_Eng...</td>\n",
       "      <td>Our next guest is sitting in the audience righ...</td>\n",
       "      <td>guest sitting audience right now you Everybody...</td>\n",
       "      <td>{'Happy': 0.24, 'Angry': 0.2, 'Surprise': 0.23...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Str...</td>\n",
       "      <td>Everywhere you go. I'm like, crazy. Well, I me...</td>\n",
       "      <td>go Im like crazy Well meant Id calm down Theyr...</td>\n",
       "      <td>{'Happy': 0.34, 'Angry': 0.03, 'Surprise': 0.1...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.\\audio\\Jordan_Peterson___How_to_Have_Better_C...</td>\n",
       "      <td>Exploration. I really thought this was interes...</td>\n",
       "      <td>Exploration thought interesting intellectual d...</td>\n",
       "      <td>{'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2,...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.\\audio\\Penn_Badgley_Can_Go_From_Charming_To_C...</td>\n",
       "      <td>Listen, everybody, you know my next guest from...</td>\n",
       "      <td>Listen everybody know guest Gossip Girl easy J...</td>\n",
       "      <td>{'Happy': 0.33, 'Angry': 0.04, 'Surprise': 0.2...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.\\audio\\Small_Talk.mp3</td>\n",
       "      <td>Excuse me. Hi. I'm trying to relax. Would you ...</td>\n",
       "      <td>Excuse me Hi Im trying relax mind Oh sorry Mr ...</td>\n",
       "      <td>{'Happy': 0.26, 'Angry': 0.02, 'Surprise': 0.1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.\\audio\\The_Oprah_Conversation_—_Will_Smith_On...</td>\n",
       "      <td>To this day, if we start talking it's 4 hours....</td>\n",
       "      <td>day start talking 4 hours 4 hours exchange sen...</td>\n",
       "      <td>{'Happy': 0.38, 'Angry': 0.04, 'Surprise': 0.1...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.\\audio\\The_Viral_Voice_that_Sounds_Like_Siri_...</td>\n",
       "      <td>My next guest blows my mind, and I'm sure she'...</td>\n",
       "      <td>guest blows mind Im sure going you Matter fact...</td>\n",
       "      <td>{'Happy': 0.14, 'Angry': 0.06, 'Surprise': 0.2...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File Name  \\\n",
       "0  .\\audio\\Bill_Gates_Chats_with_Ellen_for_the_Fi...   \n",
       "1  .\\audio\\Ellen_Taught_This_Fan_How_to_Speak_Eng...   \n",
       "2  .\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Str...   \n",
       "3  .\\audio\\Jordan_Peterson___How_to_Have_Better_C...   \n",
       "4  .\\audio\\Penn_Badgley_Can_Go_From_Charming_To_C...   \n",
       "5                             .\\audio\\Small_Talk.mp3   \n",
       "6  .\\audio\\The_Oprah_Conversation_—_Will_Smith_On...   \n",
       "7  .\\audio\\The_Viral_Voice_that_Sounds_Like_Siri_...   \n",
       "\n",
       "                                    Audio Transcript  \\\n",
       "0  I'm so happy to have you here. This is the fir...   \n",
       "1  Our next guest is sitting in the audience righ...   \n",
       "2  Everywhere you go. I'm like, crazy. Well, I me...   \n",
       "3  Exploration. I really thought this was interes...   \n",
       "4  Listen, everybody, you know my next guest from...   \n",
       "5  Excuse me. Hi. I'm trying to relax. Would you ...   \n",
       "6  To this day, if we start talking it's 4 hours....   \n",
       "7  My next guest blows my mind, and I'm sure she'...   \n",
       "\n",
       "                                    Clean Transcript  \\\n",
       "0  Im happy here time on thanks know nervous entr...   \n",
       "1  guest sitting audience right now you Everybody...   \n",
       "2  go Im like crazy Well meant Id calm down Theyr...   \n",
       "3  Exploration thought interesting intellectual d...   \n",
       "4  Listen everybody know guest Gossip Girl easy J...   \n",
       "5  Excuse me Hi Im trying relax mind Oh sorry Mr ...   \n",
       "6  day start talking 4 hours 4 hours exchange sen...   \n",
       "7  guest blows mind Im sure going you Matter fact...   \n",
       "\n",
       "                                             Emotion  topic  \n",
       "0  {'Happy': 0.31, 'Angry': 0.03, 'Surprise': 0.2...      6  \n",
       "1  {'Happy': 0.24, 'Angry': 0.2, 'Surprise': 0.23...      0  \n",
       "2  {'Happy': 0.34, 'Angry': 0.03, 'Surprise': 0.1...      2  \n",
       "3  {'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2,...      5  \n",
       "4  {'Happy': 0.33, 'Angry': 0.04, 'Surprise': 0.2...      2  \n",
       "5  {'Happy': 0.26, 'Angry': 0.02, 'Surprise': 0.1...      1  \n",
       "6  {'Happy': 0.38, 'Angry': 0.04, 'Surprise': 0.1...      2  \n",
       "7  {'Happy': 0.14, 'Angry': 0.06, 'Surprise': 0.2...      4  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4526f3",
   "metadata": {},
   "source": [
    "\n",
    " ###### Non Negative Matrix Factorization   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b9567130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ee5b2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df = 0.9, min_df= 1,stop_words ='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f5c20c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(mydf['Clean Transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5760262e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x991 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1393 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a465c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4746ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF (n_components = 7, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7b44d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(n_components=7, random_state=42)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "df32212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 15 words for Topic #0\n",
      "['year', 'huge', 'called', 'thing', 'people', 'job', 'sounds', 'billionaire', 'money', 'love', 'kids', 'right', 'kind', 'theyre', 'yeah']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #1\n",
      "['voiceover', 'hood', 'dice', 'phrases', 'cutting', 'viral', 'line', 'hey', 'going', 'said', 'siri', 'video', 'cut', 'alexa', 'light']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #2\n",
      "['adam', 'cried', 'flower', 'boring', 'brought', 'wheres', 'laughing', 'kid', 'old', 'girlfriends', 'grumpy', 'katie', 'gone', 'girlfriend', 'mr']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #3\n",
      "['love', 'dictionary', 'dominican', 'stuck', 'videos', 'hug', 'english', 'stay', 'watch', 'standing', 'affectionate', 'noun', 'ellen', 'right', 'word']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #4\n",
      "['says', 'strangers', 'yeah', 'mean', 'wow', 'say', 'meet', 'penn', 'bed', 'people', 'charming', 'joe', 'camera', 'nice', 'creepy']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #5\n",
      "['sort', 'lot', 'know', 'means', 'engaged', 'youtube', 'people', 'theres', 'conversations', 'things', 'interesting', 'better', 'think', 'maybe', 'conversation']\n",
      "\n",
      "\n",
      "The top 15 words for Topic #6\n",
      "['relationship', 'separated', 'agreed', 'self', 'versus', '100', 'cup', 'come', 'right', 'freedom', 'actually', 'broken', 'going', 'figure', 'happy']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(nmf_model.components_):\n",
    "    print(f\"The top 15 words for Topic #{i}\")\n",
    "    print([tfidf.get_feature_names()[index] for index in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "369be56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = nmf_model.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "95a92b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 6, 0, 3, 5, 1, 4, 2], dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results.argmax(axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "61964cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf['Topic'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bf6a840b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Audio Transcript</th>\n",
       "      <th>Clean Transcript</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>topic</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.\\audio\\Bill_Gates_Chats_with_Ellen_for_the_Fi...</td>\n",
       "      <td>I'm so happy to have you here. This is the fir...</td>\n",
       "      <td>Im happy here time on thanks know nervous entr...</td>\n",
       "      <td>{'Happy': 0.31, 'Angry': 0.03, 'Surprise': 0.2...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.\\audio\\Ellen_Taught_This_Fan_How_to_Speak_Eng...</td>\n",
       "      <td>Our next guest is sitting in the audience righ...</td>\n",
       "      <td>guest sitting audience right now you Everybody...</td>\n",
       "      <td>{'Happy': 0.24, 'Angry': 0.2, 'Surprise': 0.23...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Str...</td>\n",
       "      <td>Everywhere you go. I'm like, crazy. Well, I me...</td>\n",
       "      <td>go Im like crazy Well meant Id calm down Theyr...</td>\n",
       "      <td>{'Happy': 0.34, 'Angry': 0.03, 'Surprise': 0.1...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.\\audio\\Jordan_Peterson___How_to_Have_Better_C...</td>\n",
       "      <td>Exploration. I really thought this was interes...</td>\n",
       "      <td>Exploration thought interesting intellectual d...</td>\n",
       "      <td>{'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2,...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.\\audio\\Penn_Badgley_Can_Go_From_Charming_To_C...</td>\n",
       "      <td>Listen, everybody, you know my next guest from...</td>\n",
       "      <td>Listen everybody know guest Gossip Girl easy J...</td>\n",
       "      <td>{'Happy': 0.33, 'Angry': 0.04, 'Surprise': 0.2...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.\\audio\\Small_Talk.mp3</td>\n",
       "      <td>Excuse me. Hi. I'm trying to relax. Would you ...</td>\n",
       "      <td>Excuse me Hi Im trying relax mind Oh sorry Mr ...</td>\n",
       "      <td>{'Happy': 0.26, 'Angry': 0.02, 'Surprise': 0.1...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.\\audio\\The_Oprah_Conversation_—_Will_Smith_On...</td>\n",
       "      <td>To this day, if we start talking it's 4 hours....</td>\n",
       "      <td>day start talking 4 hours 4 hours exchange sen...</td>\n",
       "      <td>{'Happy': 0.38, 'Angry': 0.04, 'Surprise': 0.1...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.\\audio\\The_Viral_Voice_that_Sounds_Like_Siri_...</td>\n",
       "      <td>My next guest blows my mind, and I'm sure she'...</td>\n",
       "      <td>guest blows mind Im sure going you Matter fact...</td>\n",
       "      <td>{'Happy': 0.14, 'Angry': 0.06, 'Surprise': 0.2...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File Name  \\\n",
       "0  .\\audio\\Bill_Gates_Chats_with_Ellen_for_the_Fi...   \n",
       "1  .\\audio\\Ellen_Taught_This_Fan_How_to_Speak_Eng...   \n",
       "2  .\\audio\\Joe_Alwyn_Dishes_On_'Weird__Funny__Str...   \n",
       "3  .\\audio\\Jordan_Peterson___How_to_Have_Better_C...   \n",
       "4  .\\audio\\Penn_Badgley_Can_Go_From_Charming_To_C...   \n",
       "5                             .\\audio\\Small_Talk.mp3   \n",
       "6  .\\audio\\The_Oprah_Conversation_—_Will_Smith_On...   \n",
       "7  .\\audio\\The_Viral_Voice_that_Sounds_Like_Siri_...   \n",
       "\n",
       "                                    Audio Transcript  \\\n",
       "0  I'm so happy to have you here. This is the fir...   \n",
       "1  Our next guest is sitting in the audience righ...   \n",
       "2  Everywhere you go. I'm like, crazy. Well, I me...   \n",
       "3  Exploration. I really thought this was interes...   \n",
       "4  Listen, everybody, you know my next guest from...   \n",
       "5  Excuse me. Hi. I'm trying to relax. Would you ...   \n",
       "6  To this day, if we start talking it's 4 hours....   \n",
       "7  My next guest blows my mind, and I'm sure she'...   \n",
       "\n",
       "                                    Clean Transcript  \\\n",
       "0  Im happy here time on thanks know nervous entr...   \n",
       "1  guest sitting audience right now you Everybody...   \n",
       "2  go Im like crazy Well meant Id calm down Theyr...   \n",
       "3  Exploration thought interesting intellectual d...   \n",
       "4  Listen everybody know guest Gossip Girl easy J...   \n",
       "5  Excuse me Hi Im trying relax mind Oh sorry Mr ...   \n",
       "6  day start talking 4 hours 4 hours exchange sen...   \n",
       "7  guest blows mind Im sure going you Matter fact...   \n",
       "\n",
       "                                             Emotion  topic  Topic  \n",
       "0  {'Happy': 0.31, 'Angry': 0.03, 'Surprise': 0.2...      6      0  \n",
       "1  {'Happy': 0.24, 'Angry': 0.2, 'Surprise': 0.23...      0      6  \n",
       "2  {'Happy': 0.34, 'Angry': 0.03, 'Surprise': 0.1...      2      0  \n",
       "3  {'Happy': 0.2, 'Angry': 0.07, 'Surprise': 0.2,...      5      3  \n",
       "4  {'Happy': 0.33, 'Angry': 0.04, 'Surprise': 0.2...      2      5  \n",
       "5  {'Happy': 0.26, 'Angry': 0.02, 'Surprise': 0.1...      1      1  \n",
       "6  {'Happy': 0.38, 'Angry': 0.04, 'Surprise': 0.1...      2      4  \n",
       "7  {'Happy': 0.14, 'Angry': 0.06, 'Surprise': 0.2...      4      2  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ac51c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
